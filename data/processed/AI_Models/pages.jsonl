{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 1, "n_pages": 19, "is_scanned": false, "text": "1\nAI Models Exceed Individual Human Accuracy in Predicting\nEveryday Social Norms\nAuthors: Pontus Strimling1,2, Simon Karlsson1, Irina Vartanova1,3, Kimmo Eriksson1,4*\n1Institute for Futures Studies, Stockholm, Sweden\n2Institute for Analytical Sociology, Linköping University, Norrköping, Sweden\n3Department of Women's and Children's Health, Uppsala University\n4School of Education, Culture and Communication, Mälardalen University, Västerås, Sweden\n*Corresponding author: Kimmo Eriksson, Mälardalen University, Box 883, SE-72123\nStockholm, Sweden. E-mail: kimmo.eriksson@mdu.se\nAuthor Contributions\nPS conceived Study 1. KE conceived Study 2. IV collected the LLM data. IV and SK\nperformed the analyses with input from KE. SK drafted a first version of the introduction. KE\nwrote the full paper. All authors critically reviewed and approved the final manuscript.\nFunding\nThis research was funded by the Knut and Alice Wallenberg Foundation (grant no.\n2022.0191, recipient PS) and a Mälardalen University AI and Society Research Fellowship\n(recipient KE).\nAbstract\nA fundamental question in cognitive science concerns how social norms are acquired and\nrepresented. While humans typically learn norms through embodied social experience, we\ninvestigated whether large language models can achieve sophisticated norm understanding\nthrough statistical learning alone. Across two studies, we systematically evaluated multiple\nAI systems' ability to predict human social appropriateness judgments for 555 everyday\nscenarios by examining how closely they predicted the average judgment compared to each\nhuman participant. In Study 1, GPT-4.5’s accuracy in predicting the collective judgment on a\ncontinuous scale exceeded that of every human participant (100th percentile). Study 2\nreplicated this, with Gemini 2.5 Pro outperforming 98.7% of humans, GPT-5 97.8%, and\nClaude Sonnet 4 96.0%. Despite this predictive power, all models showed systematic,\ncorrelated errors. These findings demonstrate that sophisticated models of social cognition\ncan emerge from statistical learning over linguistic data alone, challenging strong versions of\ntheories emphasizing the exclusive necessity of embodied experience for cultural\ncompetence. The systematic nature of AI limitations across different architectures indicates\npotential boundaries of pattern-based social understanding, while the models’ ability to", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 2, "n_pages": 19, "is_scanned": false, "text": "2\noutperform nearly all individual humans in this predictive task suggests that language serves\nas a remarkably rich repository for cultural knowledge transmission.\nKeywords: social cognition, cultural learning, artificial intelligence, social norms,\ncomputational modeling, large language models\nIntroduction\nHow do cognitive systems acquire and represent knowledge about social appropriateness?\nThis fundamental question in cognitive science involves a tension between different learning\nmechanisms. Some approaches emphasize top-down processes like explicit instruction and\nsocial feedback, while others focus on bottom-up statistical learning from environmental\npatterns. This tension becomes particularly intriguing when considering whether\nsophisticated social understanding can emerge purely from pattern recognition in linguistic\ndata, without the lived experiences that many theories consider essential for cultural\ncompetence.\nResearch on statistical learning demonstrates that humans can extract complex regularities\nfrom input without explicit instruction, from basic sequence patterns in infancy to\nsophisticated linguistic structures (Elman, 1990; Saffran et al., 1996). Distributional\napproaches suggest that meaning and social knowledge can emerge from co-occurrence\npatterns in language alone, while Bayesian models show how social and pragmatic\nunderstanding can arise through probabilistic inference over communicative patterns\n(Goodman & Frank, 2016; Harris, 1954; Landauer & Dumais, 1997). These distributional and\nstatistical learning approaches also engage with theories of embodied cognition, which\npropose that conceptual understanding emerges from sensorimotor experience and bodily\ninteraction with the environment (Barsalou, 2008). Strong embodiment theories would\npredict that social understanding requires direct experience with social situations, making this\nquestion particularly theoretically significant. However, moderate embodiment theories allow\nfor abstract social knowledge to be acquired through linguistic input that describes embodied\nexperiences, which could potentially reconcile statistical learning with embodied cognition\nframeworks.\nNow consider large language models (LLMs) like GPT and Gemini. These systems wield\npowerful artificial intelligence without embodied experience, relying entirely on statistical\nlearning from text. At the same time, they are rapidly taking on important roles in the social\nworld. This situation raises a profound question: can computational systems achieve human-\nlevel understanding of our social world without the lived, physical experiences that many\ntheories consider essential?\nA particularly interesting case is whether LLMs can understand everyday social norms. These\nnorms are typically unwritten, context-dependent, and culturally variable. They require\nunderstanding not just what behaviors are possible, but what is appropriate given specific\nsocial contexts—nuanced judgments that humans make largely intuitively. These", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 3, "n_pages": 19, "is_scanned": false, "text": "3\ncharacteristics of everyday norms make them especially challenging to understand for\nsystems that rely entirely on statistical learning from text.\nBackground on AI and Social Reasoning\nRecent research has begun empirically testing AI social reasoning capabilities, but with\nimportant methodological limitations for understanding computational social cognition.\nMuch work examines AI performance on high-stakes moral dilemmas (Awad et al., 2018;\nTakemoto, 2024) or uses binary classifications of social behaviors (Forbes et al., 2021; Ziems\net al., 2023). These approaches miss the continuous, nuanced nature of everyday social\nappropriateness and typically benchmark AI against predetermined \"correct\" answers rather\nthan the natural variation in human social cognition. Moreover, this research focuses on\nmoral reasoning rather than the everyday norm understanding that governs most social\ninteraction.\nComplementing this work, studies have examined LLMs' understanding of everyday social\nnorms in physical settings. Resources such as SOCIAL-CHEM101 (Forbes et al., 2021),\nNormBank (Ziems et al., 2023), and EgoNormia (Rezaei et al., 2025) test LLMs' grasp of\ncommon social behaviors in contexts like public transport and workplace settings. These\nevaluations reveal mixed results: LLMs perform well in structured scenarios but struggle with\nsubtle contextual dependencies.\nHowever, existing research has a key limitation from a cognitive science perspective. When\ncomparing AI to human judgment, studies typically benchmark AI against predefined correct\nanswers or aggregate human averages. This approach obscures how AI performance\ncompares to typical individual humans and fails to acknowledge the natural variation in\nhuman social cognition. Understanding this variation is crucial for theories of cultural\ncompetence and social learning.\nThe Present Study\nThis paper addresses these limitations through a two-part investigation designed to\nprovide a rigorous benchmark of AI's ability to understand everyday social norms—the\nunwritten rules about how appropriate a given behavior is in a given context. Consider,\nfor example, how appropriate it is to laugh at a job interview, cry on a bus, or read in\nchurch. These judgments involve nuanced social understanding that goes far beyond\nknowing what behaviors are physically possible. Our overall approach shifts the focus\nfrom abstract morality and binary classifications to the continuous spectrum of\neveryday social appropriateness, and from aggregate averages to a novel individual-\nlevel performance comparison.\nWe leverage a large-scale dataset of human appropriateness judgments for 555\neveryday behaviors from 555 U.S. participants. This dataset provides both the cultural\nconsensus to predict and the individual variation to benchmark against.", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 4, "n_pages": 19, "is_scanned": false, "text": "4\nIn Study 1, we provide an initial proof-of-concept by evaluating the ability of GPT-4.5\nto predict the cultural consensus about everyday norms and benchmarking its\nperformance against individual human participants. This first study establishes whether\na sophisticated model of social norms can be learned from text alone.\nGPT-4.5 was a state-of-the-art model in early 2025 that was subsequently retired in\nsummer 2025. In Study 2, we examine the robustness and evolution of LLM\ncapabilities to predict everyday norms. We replicate the entire study with several next-\ngeneration models released later in 2025, including GPT-5, Claude Sonnet 4, and\nGemini 2.5 Pro. This comparative approach allows us to not only test the\ngeneralizability of our initial findings but also to explore the qualitative differences in\nthe \"social intelligence\" of different leading AI systems.\nAcross both studies, our central research question remains: if an AI system were evaluated as\njust another participant in a social cognition study, would its performance fall within the\nrange of typical human variation, or would it demonstrate a capability to model the collective\nnorm that exceeds that of a typical individual? As our results will show, the AI's grasp of our\nsocial world is not only highly accurate but, in its ability to reflect the collective consensus,\ndemonstrates a predictive accuracy that exceeds the vast majority of individual humans, a\nfinding with profound implications for cognitive science and AI development.\nMethodological Note on Task Comparison\nThe tasks for the AI and human participants are not identical: the AI is prompted to\nperform a meta-cognitive task of predicting the group average, whereas each human\nprovides their own appropriateness rating. Therefore, our test is a direct assessment of\nthe model's ability to extract a collective social signal from its training data, which is\ndistinct from possessing genuine, human-like social understanding. Nonetheless, a\ncomparison between the AI's predictive accuracy and individual humans' deviation\nfrom the mean is justified on the theoretical premise that a judgment of social\nappropriateness is not a statement of personal preference as much as it is an individual's\nreport on their perception of a shared, collective cultural norm. In this view, each\nhuman rating is an estimate of this societal standard. Therefore, while the explicit\ninstructions differ, both the AI and human participants are engaged in a process of\naccessing and representing a collective consensus, allowing for a meaningful\ncomparison of their accuracy relative to that consensus.\nThis theoretical framework is supported by extensive research in social cognition\ndemonstrating that individual appropriateness judgments are not purely idiosyncratic\npreferences but rather reflect individuals' estimates of shared cultural standards (Zou et\nal., 2009). Social psychological research on pluralistic ignorance shows that people\nconsistently attempt to infer others' attitudes when making social judgments (Miller &\nMcFarland, 1987), suggesting that appropriateness ratings inherently involve a meta-\ncognitive component similar to the explicit prediction task given to AI. Furthermore,\ndual-process theories of social cognition propose that social judgments often involve", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 5, "n_pages": 19, "is_scanned": false, "text": "5\nboth intuitive responses and deliberative estimation of social consensus (Strack &\nDeutsch, 2004). From this perspective, the AI's explicit prediction task may actually\nisolate the deliberative component that is present but confounded in human\nappropriateness ratings. While the tasks differ in their explicit instructions, both\nfundamentally require accessing and representing cultural knowledge to generate\nresponses that align with societal expectations.\nStudy 1\nMethods and Materials\nHuman Normative Data\nThe human data were sourced from a large-scale, pre-registered study of social norms in the\nUnited States (Anonymous, 2023), designed to elicit appropriateness ratings for a\ncomprehensive range of everyday behaviors.\nStimuli. The stimuli consisted of 555 unique scenarios created by pairing each of 37\neveryday behaviors with each of 15 common situations, replicating and extending Price and\nBouffard's (1974) pioneering methodology. This systematic combination allows examination\nof how the same behavior is perceived across different social contexts.\nThe 37 behaviors were: Argue, Belch, Blow one’s nose, Bring a dog, Chew gum, Cry, Curse,\nDance, Do a jigsaw puzzle, Drink water, Eat, Fart, Fiddle with one’s phone, Fight, Flirt, Hold\nhands, Jump, Kiss, Laugh, Listen to music on headphones, Mumble, Play cards, Pray audibly,\nRead, Run, Shout, Sigh, Sing, Sit on the floor/ground, Spit, Talk, Talk on the phone, Take a\nselfie, Wave to a friend, Whistle, Work on a laptop, and Write.\nThe 15 situations were: in a bar, on a bus, in church, in class, on a date, on a downtown\nsidewalk, in a dormitory lounge, in an elevator, at a family dinner, at a football game, at a job\ninterview, at the movies, in one’s own room, in the park, and in a restroom.\nParticipants. The study included 555 participants from the United States, recruited via\nProlific in January 2023. Recruitment aimed for balanced representation across gender, age,\npolitical ideology, and education level, providing a diverse sample for establishing cultural\nconsensus.\nAppropriateness Rating Scale. Following Price and Bouffard (1974), participants rated the\nperceived appropriateness of each situated behavior on a ten-point scale from 0 to 9,\nanchored at 0 = The behavior is extremely inappropriate in this situation and 9 = The\nbehavior is extremely appropriate in this situation. This continuous scale captures the\nnuanced nature of social judgments better than binary classifications.", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 6, "n_pages": 19, "is_scanned": false, "text": "6\nProcedure. To prevent participant fatigue while ensuring reliable estimates, each participant\nrated 50 randomly selected scenarios (10 behaviors × 5 situations). This design provided\napproximately 40 ratings per scenario, yielding stable measures of collective social norms\n(median standard error = 0.32, range = 0.04-0.53).\nAI Normative Data\nModel and Parameters. We used GPT-4.5 Preview ('gpt-4.5-preview-2025-02-27'), which\nwas available through OpenAI's API during our data collection period (May 5-13, 2025), but\nhas since been retired. To ensure consistent yet naturally variable responses, we used:\ntemperature=0.5 (chosen to balance consistency with natural variation), top_p=1.0,\nmax_tokens=4000.\nPrompting Procedure. For each scenario, the model received a carefully structured prompt:\n\"From various sources in our everyday lives we have all developed a subjective 'impression'\nor 'feeling' for the appropriateness of any given behavior in a particular situation. Imagine a\nnumber of people from the United States rated the appropriateness of [scenario] on the scale\nfrom 0 through 9, where 0 = The behavior is extremely inappropriate in this situation and 9 =\nThe behavior is extremely appropriate in this situation. Note that this scenario combines a\nbehavior and a situation (not a behavior and an object). Interpret it as someone performing\nthe behavior during the situation, in the way such a phrase would typically be understood. For\nexample, 'writing on a bus' means writing while riding the bus, not on its surface. Your task is\nto estimate the average rating that these U.S. respondents would give. Please use numbers\nwith up to two decimals to provide as detailed estimates as possible. Do not write any\ncomments or justifications for the estimates. Write only the estimate.\"\nThis prompt was designed to present the exact task given to human participants, ensuring\ncomparable cognitive demands and interpretation. The culturally-specific instruction was\nessential for testing whether the AI could access U.S. cultural knowledge rather than relying\non generic responses.\nMultiple Queries. To account for the stochastic nature of language models and ensure robust\nestimates, we queried the model five times for each scenario. This approach balances\ncomputational efficiency with statistical reliability, providing data to capture the model's\ncentral tendency while accounting for response variability.\nResults\nOur analysis examined how accurately GPT-4.5 could predict human judgments of social\nappropriateness, addressing both aggregate correlation and individual-level performance\ncomparison.\nCorrelation with Human Consensus", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 7, "n_pages": 19, "is_scanned": false, "text": "7\nThe model's continuous-scale predictions showed exceptionally strong correlation with\naverage human ratings, explaining 89% of the variance in human social norms (R² = 0.89).\nFigure 1 reveals tight linear correspondence across the full spectrum of appropriateness\njudgments. To ensure this finding wasn't an artifact of the continuous scale format, we\nperformed a robustness analysis by rounding AI predictions to the nearest integer before re-\nanalysis. The correlation remained virtually unchanged (R² = 0.88), confirming that the\nmodel's accuracy is robust and not dependent on rating scale precision\nFigure 1. Correlation between AI-generated appropriateness ratings and average human\nratings for 555 scenarios. Each dot represents a scenario as exemplified by the labeled dots.\nIndividual-Level Performance Comparison\nMore striking than the aggregate correlation was the AI's performance relative to individual\nhumans. Using Mean Absolute Error (MAE) from the group average as our metric, GPT-4.5's\ncontinuous predictions placed it in the 100th percentile of human accuracy, outperforming all\n555 participants (Figure 2). Even when constrained to integer ratings matching the human\nscale, the AI outperformed 553 of 555 human participants (99.6th percentile), demonstrating\nthat its superior performance reflects a genuine and powerful capability for modeling social\nnorms from linguistic data, rather than being an artifact of the rating scale. To ensure these\nfindings were not confounded by differences in scenario difficulty across the subsets rated by\ndifferent participants, we conducted a complementary analysis comparing each individual's", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 8, "n_pages": 19, "is_scanned": false, "text": "8\nMAE to the AI's MAE on the identical subset of scenarios and obtained virtually identical\nresults (Supplementary Figure 1).\nFigure 2. Distribution of Mean Absolute Error (MAE) for 555 human participants, with the\nMAE of GPT-4.5 indicated.\nAnalysis of AI-Human Divergences\nWhile overall correlation was strong, examining specific scenarios where AI judgments\ndiverged from human consensus revealed interesting patterns. Table 1 shows the ten\nscenarios with largest AI-human differences. The AI systematically underestimated the\nappropriateness of some context-specific behaviors (e.g., waving to a friend in one's own\nroom, or reading in church) while overestimating the appropriateness of others (e.g., running\nat a football game, or reading on a downtown sidewalk). These exceptions potentially reflect\ndifferences between statistical pattern recognition and experiential knowledge of social\ncontexts.\nTable 1: Top 10 Scenarios with Largest AI-Human Divergence.\nScenario\nAI\nRating\nHuman Mean\nRating\nDifference Direction\nWave to a friend in one’s own room 3.02\n7.72\n-4.70\nUnderestimated\nRead in church\n2.73\n6.43\n-3.70\nUnderestimated\nRun at a football game\n7.77\n4.55\n3.22\nOverestimated", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 9, "n_pages": 19, "is_scanned": false, "text": "9\nRead on a downtown sidewalk\n7.64\n4.45\n3.19\nOverestimated\nRun in one’s own room\n8.03\n5.15\n2.88\nOverestimated\nKiss at the movies\n7.41\n4.57\n2.84\nOverestimated\nMumble at the movies\n1.27\n4.00\n-2.73\nUnderestimated\nRead in class\n5.29\n7.89\n-2.60\nUnderestimated\nDrink water at a job interview\n7.85\n5.39\n2.46\nOverestimated\nWrite at a job interview\n7.37\n4.94\n2.44\nOverestimated\nDiscussion of Study 1\nThe results of Study 1 provide a powerful proof-of-concept, demonstrating that a\nsophisticated model of human social norms can be inferred from statistical learning over text\nalone. The finding that GPT-4.5's accuracy in predicting the U.S. cultural consensus exceeded\nthat of nearly all human participants is a significant finding in itself, supporting bottom-up\ntheories of social cognition. It establishes that the capability for high-fidelity social norm\nprediction exists in artificial systems.\nHowever, these initial findings raise several crucial questions. Is this remarkable performance\nan idiosyncratic feature of this specific model, or does it represent a more general capability\nof state-of-the-art AI? Has this capability evolved or improved in the newer, more powerful\nmodels released since our initial data collection in early 2025? Finally, do different AI\nsystems, with their unique architectures and training, exhibit qualitatively different patterns in\ntheir social judgments, even if their overall accuracy is high? To address these questions of\nrobustness, evolution, and generality, we conducted a second study.\nStudy 2\nMethods and Materials\nThe methodology for Study 2 was designed to be a direct replication and extension of Study\n1. We used the identical human normative dataset and the exact same experimental prompt as\ndetailed in Study 1 to query three new large language models. We selected these models to\ntest generalizability across the most prominent state-of-the-art LLM approaches: OpenAI's\nlatest GPT model, Anthropic's latest Claude Sonnet model, and Google's latest Gemini\nmodel. All data for Study 2 was collected between August 10-15, 2025. As in the first study,\neach of the 555 scenarios was queried five times for each model, and the responses were\naveraged to obtain a final estimate.\nModels and Parameters\nThe specific models and their parameters are detailed below.", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 10, "n_pages": 19, "is_scanned": false, "text": "10\nGPT-5: The model version was gpt-5-2025-08-07. We used parameter values top_p=1.0,\nmax tokens = 2048. The GPT-5 API did not support setting a temperature parameter.\nClaude Sonnet 4: The model version was claude-sonnet-4-20250514. We used parameter\nvalues temperature=0.25, max tokens = 2048. We adjusted the temperature value to represent\nthe smaller possible range for the parameter, which is [0, 1] for Claude Sonnet 4 and [0, 2]\nfor GPT-4.5 and Gemini 2.5 Pro.\nGemini 2.5 Pro: The model version specified was 'gemini-2.5-pro', which corresponded to\nthe latest stable release during the data collection period. We used parameter values\ntemperature=0.5, max tokens = 4096.\nResults\nThe analysis for Study 2 aimed to replicate the findings from Study 1 with a new suite of\nmodels and to conduct a comparative analysis of their performance and response patterns.\nCorrelation with Human Consensus\nAll three next-generation models showed a strong positive correlation with the human norm\nconsensus, confirming the primary finding from Study 1. See Figure 3 for an illustration of\nthe results. Note that although all models performed strongly, some were even better than\nothers: R2 values ranged from 0.82 for Claude Sonnet 4 to 0.91 for GPT-5. Furthermore, note\nthat the models exhibited qualitatively different response patterns. While both GPT models\nand Gemini 2.5 produced fine-grained continuous estimates, Claude Sonnet 4's responses\nshowed a distinct \"quantization\" effect, clustering heavily at certain values (1.25, 2.35, 3.25,\n4.25, 6.75, 7.85, 8.75). This suggests a strong internal bias towards certain numbers that may\nlimit its ability to capture the subtle, continuous nature of human social consensus.\nFigure 3. Correlation between AI-generated appropriateness ratings and average human\nratings for three next-generation models. While all models show strong correlations, GPT-5\ndemonstrates the highest accuracy, and Claude Sonnet 4 exhibits a distinct 'quantized'", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 11, "n_pages": 19, "is_scanned": false, "text": "11\nresponse pattern, with responses clustering at specific numerical values, suggesting different\ninternal representation strategies.\nIndividual-Level Performance Comparison\nTo compare the models' performance at a finer grain, we benchmarked each model against\nthe 555 individual human participants using Mean Absolute Error (MAE). As shown in Table\n2, all models performed at a level exceeding the vast majority of human participants (96% or\nmore). Interestingly, when performance was measured by MAE, GPT-5 did not reach the\nlevel of the older model GPT-4.5. As in Study 1, we verified these results were not\nconfounded by differences in scenario difficulty across participant subsets by comparing each\nindividual's MAE to each AI model's MAE on identical scenarios, yielding consistent\npatterns (Supplementary Figure 1).\nTable 2. Individual-Level Performance Comparison Across All Models.\nContinuous estimates\nInteger estimates\nAI model\nMAE\n% of humans\noutperformed\nMAE\n% of humans\noutperformed\nGPT-4.5\n0.68\n100.0\n0.75\n99.6\nGPT-5\n0.81\n99.1\n0.88\n97.8\nClaude Sonnet 4 0.87\n98.0\n0.95\n96.0\nGemini 2.5 Pro\n0.76\n99.5\n0.85\n98.7\nNote: The results for GPT-4.5 are from Study 1.\nThe performance differences across models likely reflect distinct architectural and training\napproaches. Claude Sonnet 4's quantized response pattern suggests it may use more discrete\ninternal representations or have been trained with different optimization objectives that favor\ncertain numerical values. The divergence between GPT-5's at the same time superior\ncorrelation but inferior calibration compared to GPT-4.5 indicates that newer models may\ncapture more nuanced relationships between scenarios while sacrificing precise numerical\ncalibration. This dissociation suggests that correlation and calibration may rely on different\ncomputational mechanisms, with correlation reflecting the model's understanding of relative\nnormative relationships and calibration reflecting its ability to map these relationships onto\nthe appropriate numerical scale. Systematic training differences between models may also\ncontribute to these performance patterns. However, examination of these hypotheses would\nrequire more detailed information about training procedures than is available to us.\nComparison of Divergent Scenarios\nTo understand if different AI systems make similar kinds of errors, we analyzed the\nconsistency of their most divergent scenarios. In Table 3, we used the top 10 list of errors", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 12, "n_pages": 19, "is_scanned": false, "text": "12\nfrom GPT-4.5 (see Table 1) as a reference set and examined where these specific scenarios\nranked in terms of error magnitude for the next-generation models.\nTable 3. All Models’ Prediction Errors for GPT-4.5's Top 10 Divergent Scenarios.\nScenario\nGPT-4.5\nGPT-5\nClaude\nSonnet 4\nGemini\n2.5 Pro\nWave to a friend in one’s own room\n-4.70\n1.13\n0.13\n0.70\nRead in church\n-3.70\n1.62\n-4.08\n-1.51\nRun at a football game\n3.22\n1.14\n3.30\n1.97\nRead on a downtown sidewalk\n3.19\n2.22\n-1.20\n3.07\nRun in one’s own room\n2.88\n1.99\n2.70\n1.07\nKiss at the movies\n2.84\n2.96\n2.18\n1.54\nMumble at the movies\n-2.73\n-2.08\n-2.75\n-2.96\nRead in class\n-2.60\n-0.19\n-5.74\n-4.86\nDrink water at a job interview\n2.46\n2.89\n1.46\n1.96\nWrite at a job interview\n2.44\n2.51\n-3.69\n3.06\nNote: The same scenarios as in Table 1. Entries are differences between the model prediction\nand the human consensus on the 0-9 scale. Negative values means that the model\nunderestimated the appropriateness of the scenario.\nTable 3 indicates a notable overlap in the scenarios that proved most challenging. The\nprediction errors of the new models were mostly in the same direction as the original GPT-\n4.5 error. This indicates that while overall accuracy is high, the boundaries of AI's social\nunderstanding are consistent, likely reflecting a shared limitation in reasoning about specific,\nexperiential social contexts. To test the systematic nature of these errors more rigorously, we\nalso calculated the correlations between each model's signed error terms (i.e., the deviation\nfrom the human average). As shown in Table 4, the error patterns were highly correlated\nacross all models, indicating that when one model over- or underestimated the\nappropriateness of a given scenario, the other models tended to make an error in the same\ndirection. Thus, while their overall accuracy varies, the leading AI models share a consistent\nset of limitations in their understanding of human social norms. We will return to what these\nlimitations might be in the General Discussion.\nTable 4. Correlation Matrix of AI Models' Signed Error Terms.\nGPT-4.5\nGPT-5\nClaude Sonnet\n4\nGemini 2.5\nPro\nGPT-4.5\n—\nGPT-5\n0.48\n—\nClaude Sonnet 4\n0.54\n0.29\n—", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 13, "n_pages": 19, "is_scanned": false, "text": "13\nGemini 2.5 Pro\n0.60\n0.58\n0.46\n—\nNote: Each cell contains the Pearson correlation coefficient (r) between the signed error terms\nof two models across the 555 scenarios.\nGeneral Discussion\nThis two-part study provides the first quantitative demonstration that artificial intelligence,\nthrough statistical learning alone, can achieve a level of performance at the task of predicting\nhuman social consensus that exceeds nearly all individual humans on everyday norms that are\ntypically unwritten rules. Across two studies, we found that multiple AI models can predict\nthe consensus of human social norms with an accuracy exceeding the vast majority of\nindividual humans. This result does not necessarily imply a human-like social intelligence per\nse, but it demonstrates a striking proficiency in a specific meta-cognitive task: to estimate the\ncentral tendency of collective human judgment of everyday behavior in various contexts. The\nfact that this capability can be achieved through statistical learning alone is the primary focus\nof our discussion.\nStudy 1 established a powerful proof-of-concept, showing that a state-of-the-art AI (GPT-4.5)\ncould predict U.S. social norms with remarkable accuracy (R² = 0.89). Study 2 then\ndemonstrated the robustness of this finding across multiple architectures, revealing a complex\nperformance landscape. While a next-generation model (GPT-5) achieved the highest\ncorrelation with human consensus (R² = 0.91), its predecessor (GPT-4.5) demonstrated a\nlower average error, suggesting that different performance metrics capture different facets of\nAI social intelligence. The results also show that the limitations of their social understanding\nare systematic and consistent. Together, these findings have important implications for\ntheories of social cognition and computational approaches to understanding human social\nbehavior.\nImplications for Social Learning Theory\nThe consistent high performance of multiple AI models, with several achieving correlations\nnear or above R² = 0.90, provides strong support for bottom-up theories of social learning.\nWhile many theories emphasize explicit instruction and social feedback (Bandura, 1977), our\nfindings align more closely with statistical learning approaches suggesting that complex\nsocial knowledge can be acquired through pattern extraction from environmental input\n(Saffran et al., 1996; Elman, 1990). The AIs' performance is consistent with distributional\ntheories proposing that cultural knowledge emerges from co-occurrence patterns in language\n(Harris, 1954; Landauer & Dumais, 1997) and Bayesian models of pragmatic understanding\nthrough probabilistic inference over communicative patterns (Goodman & Frank, 2016).\nThe successful replication across different architectures strengthens the claim that language\ncontains rich, structured information about social appropriateness. Indeed, the fact that an\nunderstanding of norms exceeding the predictive accuracy of individual humans can be", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 14, "n_pages": 19, "is_scanned": false, "text": "14\nachieved from linguistic data alone suggests that language may serve as a more systematic\nand robust repository for cultural transmission than many theories of cultural evolution\ntypically assume. The AI's superior ability to predict consensus, despite lacking the varied\nindividual experiences that shape human judgment, implies that collective cultural knowledge\nmay be more structured and accessible than the noise of individual variation might suggest.\nComputational Models of Social Cognition\nFrom a computational perspective, our results reveal insights about the requirements for\nsocial understanding. The qualitative differences between models are particularly valuable for\nunderstanding how different approaches shape social cognition. For instance, Claude Sonnet\n4's \"quantized\" response pattern suggests a more discrete internal representation of\nappropriateness, while the GPT models' continuous estimates indicate more analog\nprocessing of social gradations. This demonstrates that multiple computational approaches\ncan achieve high-level social understanding, even if their internal representations vary\nsubstantially.\nA key insight into the nature of AI social cognition comes from the divergence in\nperformance between correlation (R2) and mean absolute error (MAE). The results show that\nGPT-5 achieved the highest correlation, suggesting it has the most refined model of the\nrelative relationships between social norms—it best understands the ‘shape’ of the normative\nlandscape. However, its predecessor, GPT-4.5, achieved a lower MAE, indicating it is better\ncalibrated, with its predictions being, on average, closer to the precise human consensus.\nThis finding is significant from a cognitive science perspective, as it suggests that the ability\nto understand normative structures (measured by high R2) and the ability to provide perfectly\ncalibrated estimates (measured by low MAE) are distinct and dissociable computational\ncapabilities. That these abilities may not improve in lockstep during model development\nchallenges a monolithic view of \"social understanding\" and implies that different\ncomputational processes may underlie these different facets of social cognition.\nThe individual-level benchmarking methodology we introduce offers a powerful new tool for\nevaluating computational models of social cognition. By comparing AI performance to the\nfull distribution of human variation, we can assess whether an artificial system's performance\nfalls within the typical range of human variation or exceeds it—a distinction crucial for\nunderstanding the computational basis of social intelligence and for developing more\nsophisticated theories of cultural competence.\nUnderstanding the Boundaries of Statistical Social Learning\nThe error patterns were consistently positively correlated across all models, indicating that\nwhen one model made an error on a scenario, the other models tended to err in the same\ndirection. A plausible interpretation is that certain aspects of social understanding may\nrequire computational mechanisms beyond pattern recognition over text. Our analysis of\ndivergent scenarios provides suggestive evidence about the boundaries of statistical social", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 15, "n_pages": 19, "is_scanned": false, "text": "15\nlearning, though these interpretations remain necessarily speculative without direct access to\nmodel internals.\nA clear illustration emerges from the scenario of reading in church, where GPT-4.5, Claude\nSonnet 4, and Gemini 2.5 Pro all significantly underestimated its appropriateness (human\nmean = 6.53, AI range = 2.35 - 4.92). This convergent error plausibly reflects a challenge in\nresolving semantic ambiguity that depends heavily on contextual interpretation. The models\nappear to activate a general script for \"reading\" as a solitary, potentially disruptive activity\nduring a religious service. In contrast, human participants likely interpreted the phrase\nthrough a more context-specific lens, recognizing \"reading in church\" as referring to the\ncommunal and appropriate reading of religious texts such as hymns, prayers, or scripture.\nThis suggests that complete social understanding may require flexible contextual reasoning\nthat goes beyond pattern recognition over linguistic co-occurrences.\nOther systematic errors point to different types of cognitive challenges. All models\noverestimated the appropriateness of kissing at the movies (human mean = 4.57, AI range =\n6.11 - 7.53), which may reflect training data biases where literary and media representations\nover-emphasize romantic behaviors in cinema settings while underrepresenting their real-\nworld social costs to bystanders.\nConversely, the models consistently underestimated the appropriateness of mumbling at the\nmovies (human mean = 4.00, AI range = 1.04 - 1.92). While \"mumbling\" typically carries\nnegative connotations, in the specific context of a movie theater it represents the most\nsocially appropriate way to communicate without disturbing others. The models appear to\napply the behavior's general negative valence without recognizing this crucial context-\ndependent exception.\nThese examples illustrate three potential boundaries of pattern-based social understanding.\nFirst, semantic ambiguity resolution may require experiential knowledge about how\nbehaviors are actually performed in specific contexts rather than just their linguistic\ndescriptions. Second, training data biases may systematically misrepresent certain social\nsituations, particularly when media portrayals diverge from real-world social norms. Third,\ncontext-dependent valence shifts may be difficult to capture through statistical patterns alone,\nas they require understanding how situational factors can override general behavioral\nassociations.\nWhat is particularly remarkable is that these systematic limitations do not substantially\ncompromise the models' overall performance. Despite these specific challenges, all models\nachieved correlations above R² = 0.82 and outperformed the vast majority of individual\nhumans, suggesting that statistical learning can yield robust social understanding even with\nidentifiable gaps. However, the persistence of these limitations across different architectures\nand the evolution to more sophisticated models suggests they may represent genuine\nchallenges for statistical approaches to social cognition.\nLimitations and Future Directions", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 16, "n_pages": 19, "is_scanned": false, "text": "16\nAn important limitation of this study is that the norms were restricted to the U.S. cultural\ncontext. While the success of multiple models in this context provides a strong foundation\nfor understanding statistical approaches to cultural learning, systematic cross-cultural\ninvestigations are needed to test the generalizability of these computational mechanisms\nacross different cultural contexts. Cross-cultural research demonstrates variation in everyday\nnorms (Gelfand et al., 2011), and it is possible that these models would not be equally\nsuccessful in predicting culturally distant societies’ everyday norms. This is an area for future\nresearch.\nAnother limitation is that our individual-level benchmarking compared an AI performing an\nexplicit prediction task with humans performing a subjective judgment task. While we argue\nthese tasks are comparable for assessing an understanding of collective norms, future\nresearch could directly compare AI and human performance when both are given the\nidentical meta-cognitive task of predicting group averages.\nConclusion\nOur two-part study demonstrates that sophisticated models that can predict human social\nnorm consensus can emerge from statistical learning over linguistic data alone, with multiple\nAI architectures achieving an accuracy in predicting cultural consensus that exceeds the vast\nmajority of individual humans. While the models performed a meta-cognitive prediction task\ndistinct from the humans' subjective rating task, this ability to model the cultural consensus\nfrom linguistic data alone has profound implications for theories of social learning. These\nfindings support bottom-up theories of social learning while revealing systematic boundaries\nof pattern-based social understanding. The consistent limitations across different\ncomputational approaches suggest that complete social cognition may require integration of\nstatistical, experiential, and embodied forms of knowledge.\nFrom a cognitive science perspective, these results suggest that cultural knowledge\nrepresentation may be more amenable to computational approaches than previously thought,\nwhile simultaneously highlighting the computational challenges involved in achieving fully\nhuman-like social understanding. The methodological innovation of individual-level\nbenchmarking provides a new tool for investigating the computational basis of social\ncognition and for developing more sophisticated theories of how cultural competence\nemerges and operates.\nAcknowledgments\nLarge language models (Google's Gemini 2.5 Pro and Anthropic's Claude Sonnet 4) provided\nhelpful suggestions for improving the flow of the text.\nData Availability Statement\nAll data and analysis code are available at OSF\nhttps://osf.io/mydqf/?view_only=ac212881f247416c98e1baac06a478ac", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 17, "n_pages": 19, "is_scanned": false, "text": "17\nConflicts of Interest\nThe authors declare no conflicts of interest.\nReferences\nAnonymous (2023).\nAwad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., ... & Rahwan, I. (2018).\nThe moral machine experiment. Nature, 563(7729), 59-64.\nBandura, A. (1977). Social learning theory. Englewood Cliffs, NJ: Prentice Hall.\nBarsalou, L. W. (2008). Grounded cognition. Annual Review of Psychology, 59, 617-645.\nDillion, D., Mondal, D., Tandon, N., & Gray, K. (2025). AI language model rivals expert\nethicist in perceived moral expertise. Scientific Reports, 15(1), 4084.\nElman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2), 179-211.\nForbes, M., Hwang, J. D., Shwartz, V., Sap, M., & Choi, Y. (2021). Social Chemistry 101:\nLearning to reason about social and moral norms. Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 707-724.\nGelfand, M. J., Raver, J. L., Nishii, L., Leslie, L. M., Lun, J., Lim, B. C., ... & Yamaguchi, S.\n(2011). Differences between tight and loose cultures: A 33-nation study. Science, 332(6033),\n1100-1104.\nGoodman, N. D., & Frank, M. C. (2016). Pragmatic language interpretation as probabilistic\ninference. Trends in Cognitive Sciences, 20(11), 818-829.\nHarris, Z. S. (1954). Distributional structure. Word, 10(2-3), 146-162.\nHenrich, J., & McElreath, R. (2003). The evolution of cultural evolution. Evolutionary\nAnthropology, 12(3), 123-135.\nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic\nanalysis theory of acquisition, induction, and representation of knowledge. Psychological\nReview, 104(2), 211-240.\nLourie, N., Le Bras, R., & Choi, Y. (2021). Scruples: A corpus of community ethical\njudgments on 32,000 real-life anecdotes. In Proceedings of the AAAI Conference on Artificial\nIntelligence (Vol. 35, No. 15, pp. 13470-13479).\nMiller, D. T., & McFarland, C. (1987). Pluralistic ignorance: When similarity is interpreted\nas dissimilarity. Journal of Personality and Social Psychology, 53(2), 298-305.", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 18, "n_pages": 19, "is_scanned": false, "text": "18\nPrice, R. H., & Bouffard, D. L. (1974). Behavioral appropriateness and situational constraint\nas dimensions of social behavior. Journal of Personality and Social Psychology, 30(4), 579-\n586.\nRezaei, M., Fu, Y., Cuvin, P., Ziems, C., Zhang, Y., Zhu, H., & Yang, D. (2025).\nEgoNormia: Benchmarking Physical Social Norm Understanding. arXiv preprint\narXiv:2502.20490.\nSaffran, J. R., Aslin, R. N., & Newport, E. L. (1996). Statistical learning by 8-month-old\ninfants. Science, 274(5294), 1926-1928.\nStrack, F., & Deutsch, R. (2004). Reflective and impulsive determinants of social behavior.\nPersonality and Social Psychology Review, 8(3), 220-247.\nTakemoto, K. (2024). The moral machine experiment on large language models. Royal\nSociety Open Science, 11(2), 231393.\nZiems, C., Dwivedi-Yu, J., Wang, Y. C., Halevy, A., & Yang, D. (2023). NormBank: A\nknowledge bank of situational social norms. Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics, 7756-7776.\nZou, X., Tam, K. P., Morris, M. W., Lee, S. L., Lau, I. Y. M., & Chiu, C. Y. (2009). Culture\nas common sense: Perceived consensus versus personal beliefs as mechanisms of cultural\ninfluence. Journal of Personality and Social Psychology, 97(4), 579-597.", "ocr": null}
{"pdf_path": "C:\\Users\\Anurag Mishra\\aavtaar_rag\\data\\raw_pdfs\\AI_Models.pdf", "page": 19, "n_pages": 19, "is_scanned": false, "text": "19\nSupplementary Material\nSupplementary Figure 1. Boxplots showing the difference between individual human MAE\nand LLM MAE when calculated on identical scenario subsets. For each human participant,\nwe computed MAE using only the scenarios they rated and compared this to the LLM's MAE\non the same scenarios. Positive values indicate the LLM outperformed the human participant.\nThe predominantly positive differences confirm that LLMs outperformed nearly all\nindividual humans.", "ocr": null}
